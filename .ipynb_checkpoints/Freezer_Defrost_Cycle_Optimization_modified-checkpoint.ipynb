{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:25:14.186872Z",
     "iopub.status.busy": "2022-04-23T07:25:14.185892Z",
     "iopub.status.idle": "2022-04-23T07:25:20.50143Z",
     "shell.execute_reply": "2022-04-23T07:25:20.500537Z",
     "shell.execute_reply.started": "2022-04-23T07:25:14.18682Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from pandas import read_csv\n",
    "import datetime\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta,date\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/CNQL6FZ01_5-16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.t_stamp = pd.to_datetime(df.t_stamp)\n",
    "df = df.set_index('t_stamp')\n",
    "df.index.names = ['t_stamp']\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from meteostat import Point, Hourly,Daily\n",
    "#https://dev.meteostat.net/formats.html#meteorological-data-units\n",
    "#https://dev.meteostat.net/api/stations/hourly.html#endpoint\n",
    "\n",
    "# Set time period\n",
    "start = min(df.index)\n",
    "end = max(df.index)\n",
    "\n",
    "# Create Point\n",
    "location = Point(36.31874, -94.1208)\n",
    "\n",
    "# Get daily data for 2018\n",
    "weather_hr = Hourly(location, start, end)\n",
    "weather_hr = weather_hr.fetch()\n",
    "\n",
    "weather_d = Daily(location, start, end)\n",
    "weather_d = weather_d.fetch()\n",
    "\n",
    "#Date and time statements follow the ISO 8601 standard \n",
    "#(e.g. 2016-12-31 for December 31st 2016 and 23:59:58 for 23 hours, 59 minutes, and 58 seconds). \n",
    "#The time zone used by Meteostat is Coordinated Universal Time (UTC).\n",
    "#Coordinated Universal Time is 5 hours ahead of Central Time\n",
    "weather_hr.index = weather_hr.index - timedelta(hours=5)\n",
    "#data[data.index == '2023-04-27 14:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hr = weather_hr[['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'pres', 'coco']]\n",
    "weather_d = weather_d[['tavg', 'tmin', 'tmax', 'prcp', 'wdir', 'wspd', 'pres']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_d.rename(columns={i:i+'_d' for i in weather_d.columns}, inplace=True)\n",
    "weather_hr.rename(columns={i:i+'_hr' for i in weather_hr.columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hr.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove columns with std == 0 (not changing)\n",
    "df = df.transpose()[(df.std()!=0)].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge_asof(df,weather_hr,left_on=df.index, right_on=weather_hr.index).set_index('key_0')\n",
    "df = pd.merge_asof(df,weather_d,left_on=df.index, right_on=weather_d.index).set_index('key_0')\n",
    "df.index.names = ['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['No_Defrost'] = np.where((df['CoilA_Defrost'] == 0.0)&\n",
    "                            (df['CoilB_Defrost'] == 0.0)&\n",
    "                            (df['CoilC_Defrost'] == 0.0)&\n",
    "                            (df['CoilD_Defrost'] == 0.0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp at decision begin\n",
    "def value_at_decision_begin(df,list_of_features):\n",
    "    df['Time_From_Decision_Begin'] = 0\n",
    "    dates = np.unique([i.strftime('%m/%d/%Y') for i in df.index.date])\n",
    "    for l in list_of_features:\n",
    "        df[f'{l}_Decision_Begin'] = 0\n",
    "        for date in dates:\n",
    "            for n in range(0,6):\n",
    "                dt_0 = pd.to_datetime(f\"{date} 07:30:00.000\") + timedelta(hours=n*2.5)\n",
    "                dt_1 = dt_0+timedelta(hours=2.5)\n",
    "                # since optimization take 9 - 10 mins to complete for every 2.5 hr, we subtract 0.1667 from\n",
    "                df[f'{l}_Decision_Begin'][dt_0- timedelta(hours=0.1667):dt_1] = df[f'{l}'][df.index.get_loc(dt_0 - timedelta(hours=0.1667), method='nearest')]\n",
    "    for date in dates:\n",
    "        for n in range(0,6):\n",
    "            dt_0 = pd.to_datetime(f\"{date} 07:30:00.000\") + timedelta(hours=n*2.5)\n",
    "            dt_1 = dt_0+timedelta(hours=2.5)\n",
    "            df['Time_From_Decision_Begin'][dt_0 - timedelta(hours=0.1667):dt_1] = list(range(len(df['Time_From_Decision_Begin'][dt_0 - timedelta(hours=0.1667):dt_1])))\n",
    "    \n",
    "    return df\n",
    "df = value_at_decision_begin(df,['ReturnAir_Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_continuous_ones(arr):\n",
    "    l = [0]*len(arr)\n",
    "    count = 0\n",
    "    for i in range(1,len(arr)):\n",
    "        if arr[i-1] == 1 and arr[i] == 1:\n",
    "            count += 1\n",
    "            l[i] = count\n",
    "        else:\n",
    "            count = 0\n",
    "            \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CoilA_Defrost = count_continuous_ones(list(df.CoilA_Defrost))\n",
    "df.CoilB_Defrost = count_continuous_ones(list(df.CoilB_Defrost))\n",
    "df.CoilC_Defrost = count_continuous_ones(list(df.CoilC_Defrost))\n",
    "df.CoilD_Defrost = count_continuous_ones(list(df.CoilD_Defrost))\n",
    "df.No_Defrost = count_continuous_ones(list(df.No_Defrost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStart='07:30'\n",
    "timeEnd='22:30'\n",
    "\n",
    "df['date'] = df.index.date\n",
    "df['dayOfWeek'] = df.index.dayofweek\n",
    "df['hrOfDay'] = df.index.hour\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.dayOfWeek != 5) & \n",
    "        (df.dayOfWeek != 6) &\n",
    "        (df.date != date(2023, 4, 13)) &\n",
    "        (df.date != date(2023, 4, 17)) &\n",
    "        (df.date != date(2023, 4, 22)) &\n",
    "        (df.date != date(2023, 4, 29)) &\n",
    "        (df.date != date(2023, 5, 6)) &\n",
    "        (df.date != date(2023, 5, 7)) &\n",
    "\n",
    "        (df.date != date(2023, 5, 13)) &\n",
    "        (df.date != date(2023, 5, 14)) &\n",
    "\n",
    "        (df.date != date(2023, 5, 19)) &\n",
    "        (df.date != date(2023, 5, 20)) &\n",
    "        (df.date != date(2023, 5, 21)) &\n",
    "\n",
    "        (df.date != date(2023, 5, 27)) &\n",
    "        (df.date != date(2023, 5, 28)) &\n",
    "        (df.date != date(2023, 5, 29)) &\n",
    "        (df.date != date(2023, 6, 3)) &\n",
    "        (df.date != date(2023, 6, 4)) &\n",
    "        (df.date != date(2023, 6, 5)) &\n",
    "\n",
    "        (df.date != date(2023, 6, 10)) &\n",
    "        (df.date != date(2023, 6, 11)) &   \n",
    "        (df.date != date(2023, 6, 20)) &\n",
    "\n",
    "     (df.date < date(2023, 7, 1))]\n",
    "\n",
    "df = df.between_time(timeStart, timeEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.groupby(['date']).std().CoilA_Defrost < 6\n",
    "print(s[s].index)\n",
    "df = df[~df['date'].isin(s[s].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "        #Target:\n",
    "        'ReturnAir_Temp',\n",
    "        \n",
    "        #Decision:\n",
    "        'CoilA_Defrost', 'CoilB_Defrost', 'CoilC_Defrost','CoilD_Defrost', 'No_Defrost',\n",
    "        \n",
    "        #Environment:\n",
    "        'ReturnAir_Temp_Decision_Begin',\n",
    "        'Time_From_Decision_Begin',\n",
    "        'hrOfDay',\n",
    "        'dayOfWeek', \n",
    "        \n",
    "        #Weather:\n",
    "        'pres_hr','rhum_hr','temp_hr', 'prcp_hr', 'wdir_hr', 'wspd_hr', #'dwpt_hr', #'coco_hr',\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analytics (Data Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Map for numeric variables:\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11,5)})\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "\n",
    "sns.lineplot(data=df, x='hrOfDay', y=\"ReturnAir_Temp\", hue=\"dayOfWeek\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1),  title = 'dayOfWeek, Monday = 0',loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df.values\n",
    "# specify columns to plot\n",
    "#3,5\n",
    "groups = [0,1,2,3,4,5]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure(figsize=(12,7))\n",
    "for group in groups:\n",
    "    pyplot.subplot(len(groups), 1, i)\n",
    "    pyplot.plot(values[:, group])\n",
    "    pyplot.title(df.columns[group], y=\n",
    "                 0.7, loc='right',size = 12)\n",
    "    i += 1\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,3)})\n",
    "sns.set_style(\"white\")\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "y1 = list(df[\"ReturnAir_Temp\"])\n",
    "y2 = list(df[\"pres_hr\"])\n",
    "\n",
    "        \n",
    "ax1.plot(y1, 'silver')\n",
    "ax2.plot(y2, 'b-')\n",
    "\n",
    "ax1.set_xlabel('Time (Hr)')\n",
    "ax1.set_ylabel('Return Temp Hourly', color='silver')\n",
    "ax2.set_ylabel('Dew Point Hourly', color='b')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics (XGBOOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['ReturnAir_Temp']\n",
    "X = df.drop(columns=['ReturnAir_Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data based on the date\n",
    "split_date=\"05-09-2023\"\n",
    "X_train=X.loc[X.index <=split_date].copy()\n",
    "y_train=y.loc[y.index <=split_date].copy()\n",
    "X_test=X.loc[X.index >split_date].copy()\n",
    "y_test=y.loc[y.index >split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_orig=X.loc[X.index >split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "X_train[X_train.columns] = mm.fit_transform(X_train[X_train.columns])\n",
    "X_test[X_test.columns] = mm.transform(X_test[X_test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam Tuning XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "#Import 'scope' from hyperopt in order to \n",
    "#obtain int values for certain hyperparameters.\n",
    "from hyperopt.pyll.base import scope\n",
    "hyperparameter_grid={'max_depth': scope.int(hp.quniform(\"max_depth\", 1, 15, 1)),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 1000,\n",
    "        'eta': hp.uniform('eta', 0,1),            \n",
    "        'seed': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    model = xgb.XGBRegressor(**space, early_stopping_rounds=100,  eval_metric=\"rmse\")\n",
    "    #Define evaluation datasets.\n",
    "    evaluation = [(X_train, y_train), (X_test, y_test)]\n",
    "    #Fit the model. \n",
    "    model.fit(X_train, y_train,\n",
    "            eval_set=evaluation,\n",
    "            verbose=False)\n",
    "    pred = model.predict(X_test)\n",
    "    mse= mean_squared_error(y_test, pred)\n",
    "    print (\"SCORE:\", mse)\n",
    "    return {'loss':mse, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = hyperparameter_grid,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 25,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trials.results[np.argmin([r['loss'] for r in \n",
    "    trials.results])]['model']\n",
    "lowest_loss = trials.results[np.argmin([r['loss'] for r in \n",
    "    trials.results])]['loss']\n",
    "print(best_model, lowest_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned=xgb.XGBRegressor(\n",
    "                    max_dept=int(best_hyperparams['max_depth']),\n",
    "                    #colsample_bytree=best_hyperparams['colsample_bytree'],\n",
    "                    objective='reg:squarederror',\n",
    "                    tree_method='hist',\n",
    "                    eval_metrics='rmse',\n",
    "                    eta=best_hyperparams['eta'],\n",
    "                    gamma=best_hyperparams['gamma'],\n",
    "                    min_child_weight=best_hyperparams['min_child_weight'],\n",
    "                    early_stopping_rounds=500,\n",
    "                    n_estimators=1000,\n",
    ")\n",
    "xgb_tuned.fit(X_train, y_train,\n",
    "              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "              verbose=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clrs = ['red' if (x<288) and (x>100) else 'grey' for x in ]\n",
    "_=plot_importance(xgb_tuned, height=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = xgb_tuned.predict(X_test)\n",
    "xgb_score = mean_squared_error(y_test, xgb_preds, squared=False)\n",
    "print('RMSE_Best_Model:', xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "sns.set_style(\"white\")\n",
    "i = randint(0,len(y_test)-200)\n",
    "\n",
    "pyplot.figure(figsize=(20,5))\n",
    "pyplot.plot(list(y_test)[i:i+200], label='return air temp')\n",
    "pyplot.plot(xgb_preds[i:i+200], label='predicted return air temp')\n",
    "plt.xlabel('Time (min)', fontsize=18)\n",
    "plt.ylabel('Temperature in F', fontsize=16)\n",
    "plt.rc('legend', fontsize = 16)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyplot.figure(figsize=(20,5))\n",
    "pyplot.plot(list(y_test)[0:-1], label='return air temp')\n",
    "pyplot.plot(xgb_preds[0:-1], label='predicted return air temp')\n",
    "plt.xlabel('Time (min)', fontsize=18)\n",
    "plt.ylabel('Temperature in F', fontsize=16)\n",
    "plt.rc('legend', fontsize = 16)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prescriptive Analytics (Integer Programming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_defrost_temp(i):\n",
    "    # this function find the instruction and transformed to the way prediciton model wants to see. \n",
    "    n=int(np.argmax(i[0:5]))\n",
    "    timer = i[5]\n",
    "    mx = np.array([list(i)]*timer)\n",
    "    mx[:, n] = [j for j in range(0,timer)]\n",
    "    mx = np.delete(mx, 5, 1)\n",
    "    mx = mm.transform(mx)\n",
    "    xgb_preds = xgb_tuned.predict(mx)\n",
    "    e = mean_squared_error([TARGET_TEMP]*len(xgb_preds), xgb_preds, squared=True)\n",
    "    return xgb_preds[-1],xgb_preds,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_cost(s):\n",
    "    costs = {}\n",
    "    temps = {}\n",
    "    for (i,j) in s:\n",
    "        costs[(i,j)] = end_defrost_temp(i[:-1])[2]\n",
    "        temps[(i,j)] = end_defrost_temp(i[:-1])[1]\n",
    "    return costs,temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = [[1., 0., 0., 0., 0., 35],\n",
    "             [0., 1., 0., 0., 0., 35],\n",
    "             [0., 0., 1., 0., 0., 35],\n",
    "             [0., 0., 0., 1., 0., 35],\n",
    "             [0., 0., 0., 0., 1.,  5],\n",
    "             [0., 0., 0., 0., 1., 10],\n",
    "             [0., 0., 0., 0., 1., 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = np.unique([i.strftime('%m/%d/%Y') for i in X_test.index.date])\n",
    "TARGET_TEMP = -25\n",
    "solutions = {}\n",
    "dd = 0 #unique id for each generated node\n",
    "for date in dates:\n",
    "    dt_0 = pd.to_datetime(f\"{date} 07:30:00.000\")\n",
    "    while dt_0 < pd.to_datetime(f\"{date} 22:30:00.000\"):\n",
    "        ############## Create Network Flow Structure ###############\n",
    "        start = time.time()\n",
    "        print(f'Working on {dt_0}:')\n",
    "        print('Creating network...')\n",
    "        # This is the starting node for each batch of optimization network. The node starts with index 5 to include present tempreture.\n",
    "        node_0 = X_test_orig.iloc[X_test_orig.index.get_loc(dt_0, method='nearest')][5:]      \n",
    "        if dt_0 == pd.to_datetime(f\"{date} 07:30:00.000\"):\n",
    "            #Assume Coil A defrost first everyday, which is (1,0,0,0,0....), dd is unique id for each generated node\n",
    "            node_0 = [tuple(list(decisions[0])+list(node_0)+[dd])]\n",
    "            dd = dd+1\n",
    "        \n",
    "        else:\n",
    "            # If not first node of that day that means we need to use previous solution's last node, which will always end with the word sink.\n",
    "            decision_prev = [i for (i,j) in Arcs if select[(i,j)].solution_value == 1 and j == 'sink']\n",
    "            n=int(np.argmax(decision_prev[0][0:5]))\n",
    "            node_0 = [tuple(list(decisions[n])+list(node_0)+[dd])]\n",
    "            dd = dd+1\n",
    "        # The following code is to generate time interval, right now it's hard code to look ahead 2.5 hours, which means we need 5 x 30mins intervals\n",
    "        for t in range(1,6):\n",
    "            globals()[f'dt_{t}'] = globals()[f'dt_{t-1}'] + timedelta(hours=0.5)\n",
    "            globals()[f'node_{t}'] = X_test_orig.iloc[X_test_orig.index.get_loc(globals()[f'dt_{t}'], method='nearest')][6:]\n",
    "\n",
    "        # This part connects the nodes in a dictionary form: The key is the origin node and the values in the dict are what the origin connects to. \n",
    "        node_1_dict = {}\n",
    "        for n in node_0:\n",
    "            node_1_dict[n] = [tuple(list(i)+[end_defrost_temp(n[:-1])[0]]+list(node_1)+[dd]) for i in decisions]\n",
    "            dd = dd+1\n",
    "        for p in range(2,6):\n",
    "            globals()[f'node_{p}_dict'] = {}\n",
    "            for q in globals()[f'node_{p-1}_dict'].keys():\n",
    "                for n in globals()[f'node_{p-1}_dict'][q]:\n",
    "                    globals()[f'node_{p}_dict'][n] = [tuple(list(i)+[end_defrost_temp(n[:-1])[0]]+list(globals()[f'node_{p}']) + [dd]) for i in decisions]\n",
    "                    dd = dd+1\n",
    "        \n",
    "        # Unpacking dict into list of Arcs. \n",
    "        s0 = [(i,j) for i in node_1_dict.keys() for j in node_1_dict[i]]\n",
    "        s1 = [(i,j) for i in node_2_dict.keys() for j in node_2_dict[i]]\n",
    "        s2 = [(i,j) for i in node_3_dict.keys() for j in node_3_dict[i]]\n",
    "        s3 = [(i,j) for i in node_4_dict.keys() for j in node_4_dict[i]]\n",
    "        s4 = [(i,j) for i in node_5_dict.keys() for j in node_5_dict[i]]\n",
    "        s5 = [(j,'sink') for (i,j) in s4]\n",
    "    \n",
    "        Arcs = set(s0 + s1 + s2 + s3 + s4 + s5)\n",
    "        \n",
    "        print('Almost there..')\n",
    "        Arcs_cost, Arcs_temp = arc_cost(Arcs)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(f'Boom! Number of arcs generated is {len(Arcs_cost)}, total time spent creating arcs is {round(end-start)} s')\n",
    "        \n",
    "        ############## Define Node Supply ###############\n",
    "        start = time.time()\n",
    "        print('Setting network flow node supplies')\n",
    "        Supply = {}\n",
    "        for (i,j) in Arcs:\n",
    "            #if i!=j:\n",
    "                Supply[i] = 0\n",
    "                Supply[j] = 0\n",
    "\n",
    "        Supply[node_0[0]] = -1\n",
    "        Supply['sink'] = 1\n",
    "\n",
    "        from docplex.mp.model import Model\n",
    "        mdl = Model()\n",
    "        mdl.clear_constraints()\n",
    "        mdl.remove_objective()\n",
    "        print('Defining variables and objective')\n",
    "        #Define Variables\n",
    "        #select = mdl.continuous_var_dict(Arcs, lb=0, name='select')\n",
    "        select = mdl.binary_var_dict(Arcs, lb=0, name='select')\n",
    "\n",
    "        # objective\n",
    "        mdl.minimize(mdl.sum(Arcs_cost[i]*select[i] for i in Arcs))\n",
    "    \n",
    "        ############## Add Flow Conservation Constraints ###############\n",
    "        print('Adding flow conservation constraints..')\n",
    "        con ={}\n",
    "        for j in Supply.keys():\n",
    "            inflow = mdl.sum(select[i,j] for i in Supply.keys() if (i,j) in Arcs)\n",
    "            outflow = mdl.sum(select[j,i] for i in Supply.keys() if (j,i) in Arcs)\n",
    "            con[j] = mdl.add_constraint(inflow - outflow == Supply[j])\n",
    "\n",
    "        coil_defrost = {}\n",
    "        coil_defrost_time = {}\n",
    "        end = time.time()\n",
    "        print(f'Total time adding flow objective and constraints is {round(end - start)} s')\n",
    "        \n",
    "        ############## Add Node Specific Constraints ###############\n",
    "        print('Adding node contraints...')\n",
    "        start = time.time()\n",
    "        # Adding constraints: Each Coil defrost at least once in the current period or the last. \n",
    "        # If time is the beginning of the day, don't look for last period:\n",
    "        if dt_0 == pd.to_datetime(f\"{date} 07:30:00.000\"):\n",
    "            for w in range(0,4):\n",
    "                coil_defrost[w]= mdl.add_constraint(mdl.sum(select[i,j]*i[w] for (i,j) in Arcs) <= 2)\n",
    "            mdl.add_constraint(mdl.sum(select[i,j]*i[5] for (i,j) in Arcs) == 150)\n",
    "        else:\n",
    "            previous_time = dt_0 - timedelta(hours=2.5)\n",
    "            previous_time_sol = [i[:6] for (i,j) in list(solutions[previous_time].keys())]\n",
    "            previous_time_sol = np.array([list(i) for i in previous_time_sol]).sum(axis = 0)\n",
    "            previous_time_sol = [int(i) for i in previous_time_sol]\n",
    "            for w in range(0,4):\n",
    "                coil_defrost[w]= mdl.add_constraint(mdl.sum(select[i,j]*i[w] for (i,j) in Arcs) + previous_time_sol[w] >= 1)\n",
    "                #coil_defrost[w]= mdl.add_constraint(mdl.sum(select[i,j]*i[w] for (i,j) in Arcs) + previous_time_sol[w] <= 2)\n",
    "            mdl.add_constraint(mdl.sum(select[i,j]*i[5] for (i,j) in Arcs) + previous_time_sol[5] == 300)\n",
    "        end = time.time()\n",
    "        ############## Solve ###############\n",
    "        print(f'Total time adding node constraints is {round(end - start,3)} s')\n",
    "        start = time.time()\n",
    "        print('Solving...')\n",
    "        mdl.solve()\n",
    "        end = time.time()\n",
    "        print(f'Solved...total solving time is {round(end - start,3)} s')\n",
    "        print('\\n')\n",
    "        solutions[dt_0] = {(i,j):Arcs_temp[(i,j)] for (i,j) in Arcs if select[(i,j)].solution_value == 1}\n",
    "        #set initial time of the next optimization period\n",
    "        dt_0 = dt_0 + timedelta(hours=2.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.print_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.get_solve_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Solution for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "dates = np.unique([i.strftime('%m/%d/%Y') for i in X_test.index.date])\n",
    "#dates = ['2023-05-01']\n",
    "for date in dates:\n",
    "    for n in range(0,6):\n",
    "        time = pd.to_datetime(f\"{date} 07:30:00.000\") + timedelta(hours=n*2.5)\n",
    "        \n",
    "        start_id = min([i[-1] for (i,j) in list(solutions[time].keys())])\n",
    "        start = [(i,j) for (i,j) in list(solutions[time].keys()) if i[-1] == start_id][0]\n",
    "        \n",
    "        l = l + list(solutions[time][start])\n",
    "        i = start\n",
    "\n",
    "        while i[1] != 'sink':\n",
    "            for j in solutions[time].keys():\n",
    "                if i != j:\n",
    "                    if i[1] == j[0]:\n",
    "                        l = l + list(solutions[time][j])\n",
    "                        i = j\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_0 = np.std(list(xgb_preds)[0:len(l)])\n",
    "print(f'Predicted Temp Existing Defrost Sequence Standard Deviation: {sd_0:.2f}')\n",
    "\n",
    "sd_2 = np.std(list(y_test)[0:len(l)])\n",
    "print(f'Temp Actual Standard Deviation: {sd_2:.2f}')\n",
    "\n",
    "sd_1 = np.std(l)\n",
    "print(f'Predicted Temp Optimized Defrost Sequence Standard Deviation: {sd_1:.2f}')\n",
    "\n",
    "percent_reduction = round((sd_0 - sd_1)*100/sd_0,2)\n",
    "print(f'Standard Deviation Reduction After Optimization: {percent_reduction}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyplot.figure(figsize=(20,6))\n",
    "pyplot.plot(list(y_test)[0:len(l)], color='gainsboro', label=f'Actual return air temp sd {sd_2:.2f}')\n",
    "pyplot.plot(list(xgb_preds)[0:len(l)],color='silver',  label=f'Prdicted return air temp, sd: {sd_0:.2f}')\n",
    "\n",
    "\n",
    "pyplot.plot(l,color='dodgerblue', label=f'Optimized return air temp sd: {sd_1:.2f}')\n",
    "plt.xlabel('Time (min)', fontsize=18)\n",
    "plt.ylabel('Temperature in F', fontsize=16)\n",
    "plt.title(f'Optimized Return Air Temp Reduced Standard Deviation (SD) by {percent_reduction}%',fontsize = 20)\n",
    "plt.rc('legend', fontsize = 16)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "dates = np.unique([i.strftime('%m/%d/%Y') for i in X_test.index.date])\n",
    "#dates = ['2023-05-01']\n",
    "for date in dates:\n",
    "    for n in range(0,6):\n",
    "        time = pd.to_datetime(f\"{date} 07:30:00.000\") + timedelta(hours=n*2.5)\n",
    "        \n",
    "        d = d + sorted([i[:6] for (i,j) in list(solutions[time].keys())], key=lambda x: x[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Decisions Into Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "def convert_to_timeSequence(instruction_list,n):\n",
    "    coil_instr = []\n",
    "    for i in instruction_list:\n",
    "        if np.argmax(i[:5]) == n:\n",
    "            if np.argmax(i) != 4:\n",
    "                coil_instr = coil_instr + list(repeat(1, 35))\n",
    "            else:\n",
    "                coil_instr = coil_instr + list(repeat(1, i[5]))\n",
    "        elif np.argmax(i) == 4:\n",
    "            coil_instr = coil_instr + list(repeat(0, i[5])) \n",
    "        else:\n",
    "            coil_instr = coil_instr + list(repeat(0, 35))\n",
    "        \n",
    "    return coil_instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoilA_instructions = convert_to_timeSequence(d,0)\n",
    "CoilB_instructions = convert_to_timeSequence(d,1)\n",
    "CoilC_instructions = convert_to_timeSequence(d,2)\n",
    "CoilD_instructions = convert_to_timeSequence(d,3)\n",
    "NoDefrost_instructions = convert_to_timeSequence(d,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoilA_instructions = count_continuous_ones(CoilA_instructions)\n",
    "CoilB_instructions = count_continuous_ones(CoilB_instructions)\n",
    "CoilC_instructions = count_continuous_ones(CoilC_instructions)\n",
    "CoilD_instructions = count_continuous_ones(CoilD_instructions)\n",
    "NoDefrost_instructions = count_continuous_ones(NoDefrost_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date=\"05-09-2023\"\n",
    "X_test_orig=df.loc[df.index > split_date].copy()\n",
    "ReturnAir_Temp = list(X_test_orig['ReturnAir_Temp'])\n",
    "CoilA_Defrost = list(X_test_orig['CoilA_Defrost'])\n",
    "CoilB_Defrost = list(X_test_orig['CoilB_Defrost'])\n",
    "CoilC_Defrost = list(X_test_orig['CoilC_Defrost'])\n",
    "CoilD_Defrost = list(X_test_orig['CoilD_Defrost'])\n",
    "No_Defrost = list(X_test_orig['No_Defrost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instructions = pd.DataFrame(list(zip(ReturnAir_Temp,l,CoilA_instructions, CoilB_instructions,CoilC_instructions,CoilD_instructions,NoDefrost_instructions,\n",
    "                                        CoilA_Defrost,CoilB_Defrost,CoilC_Defrost,CoilD_Defrost,No_Defrost)),\n",
    "                                columns =['ReturnAir_Temp','Optimized_ReturnAir_Temp','CoilA_instructions', 'CoilB_instructions','CoilC_instructions','CoilD_instructions','NoDefrost_instructions',\n",
    "                                         'CoilA_Defrost','CoilB_Defrost','CoilC_Defrost','CoilD_Defrost','No_Defrost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Optimized Decisions with Actual Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_instructions.values\n",
    "# specify columns to plot\n",
    "#3,5\n",
    "groups = [1,2,3,4,5,6]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure(figsize=(12,6))\n",
    "for group in groups:\n",
    "    pyplot.subplot(len(groups), 1, i)\n",
    "    pyplot.plot(values[:, group])\n",
    "    pyplot.title(df_instructions.columns[group], y=0.7, loc='right',size = 12)\n",
    "    i += 1\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_instructions.values\n",
    "# specify columns to plot\n",
    "#3,5\n",
    "groups = [0,7,8,9,10,11]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure(figsize=(12,6))\n",
    "for group in groups:\n",
    "    pyplot.subplot(len(groups), 1, i)\n",
    "    pyplot.plot(values[:, group])\n",
    "    pyplot.title(df_instructions.columns[group], y=0.7, loc='right',size = 12)\n",
    "    i += 1\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
